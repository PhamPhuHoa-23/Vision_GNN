# Enhanced ViGNN-CLIP Training

PhiÃªn báº£n cáº£i tiáº¿n cá»§a ViGNN-CLIP vá»›i **Sentence-BERT** cho text embedding vÃ  **Pyramid ViG-S** cho image embedding.

## ğŸŒŸ TÃ­nh nÄƒng má»›i

### ğŸ”§ **Cáº£i tiáº¿n chÃ­nh:**
- **Sentence-BERT**: Thay tháº¿ simple text encoder báº±ng pre-trained Sentence-BERT models
- **Enhanced Pyramid ViG**: Cáº£i tiáº¿n Pyramid ViG vá»›i graph attention vÃ  adaptive pooling
- **Advanced Loss Functions**: Multi-scale contrastive loss vá»›i local vÃ  global components
- **Memory Efficient**: Tá»‘i Æ°u hÃ³a cho single GPU training
- **Comprehensive Evaluation**: ÄÃ¡nh giÃ¡ toÃ n diá»‡n vá»›i retrieval metrics

### ğŸ“Š **Kiáº¿n trÃºc:**
```
Image â†’ Enhanced PVigS â†’ Graph Attention â†’ Adaptive Pooling â†’ [512-dim vector]
                                                                      â†“
                                                              Contrastive Loss
                                                                      â†‘
Text â†’ Sentence-BERT â†’ Projection Layer â†’ LayerNorm â†’ [512-dim vector]
```

## ğŸš€ CÃ i Ä‘áº·t

### 1. **Dependencies:**
```bash
pip install -r requirements_enhanced.txt
```

### 2. **Sentence-BERT models (tá»± Ä‘á»™ng download):**
- `all-MiniLM-L6-v2`: 384D, nhanh, hiá»‡u quáº£
- `all-mpnet-base-v2`: 768D, cháº­m hÆ¡n nhÆ°ng tá»‘t hÆ¡n
- `paraphrase-multilingual-MiniLM-L12-v2`: Äa ngÃ´n ngá»¯

## ğŸ“‚ Chuáº©n bá»‹ dá»¯ liá»‡u

### **Supported formats:**

#### **Format 1: CSV (LAION-style)**
```
data/
â”œâ”€â”€ metadata.csv          # columns: image_file, caption, url, similarity
â””â”€â”€ images/
    â”œâ”€â”€ image1.jpg
    â”œâ”€â”€ image2.jpg
    â””â”€â”€ ...
```

#### **Format 2: JSON (COCO-style)**
```
data/
â”œâ”€â”€ annotations.json      # COCO format hoáº·c simple [{image: "...", caption: "..."}]
â””â”€â”€ images/
    â”œâ”€â”€ image1.jpg
    â””â”€â”€ ...
```

#### **Format 3: Text file**
```
data/
â”œâ”€â”€ train.txt            # image_path<TAB>caption
â””â”€â”€ images/
    â””â”€â”€ ...
```

#### **Format 4: Directory structure**
```
data/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â””â”€â”€ image2.jpg
â””â”€â”€ captions/
    â”œâ”€â”€ image1.txt       # Caption for image1.jpg
    â””â”€â”€ image2.txt       # Caption for image2.jpg
```

## ğŸ‹ï¸ Training

### **Basic training:**
```bash
python vignn_clip_training_script_improved.py \
    --data_path "/path/to/dataset" \
    --model_name pvig_s_224_gelu \
    --text_model all-MiniLM-L6-v2 \
    --embedding_dim 512 \
    --batch_size 32 \
    --epochs 100 \
    --lr 1e-4 \
    --output_dir "./outputs"
```

### **Advanced training with pretrained weights:**
```bash
python vignn_clip_training_script_improved.py \
    --data_path "/path/to/dataset" \
    --model_name pvig_s_224_gelu \
    --text_model all-MiniLM-L6-v2 \
    --embedding_dim 512 \
    --pretrained_vignn "pretrained_weights/pvig_s_82.1.pth.tar" \
    --batch_size 32 \
    --epochs 100 \
    --lr 1e-4 \
    --weight_decay 0.05 \
    --temperature 0.07 \
    --lambda_local 0.1 \
    --lambda_global 0.05 \
    --augmentation medium \
    --scheduler cosine \
    --gradient_clip_val 1.0 \
    --output_dir "./outputs" \
    --log_wandb \
    --save_every 10
```

### **Using the training script:**
```bash
# Chá»‰nh sá»­a DATA_PATH trong file train_enhanced.sh
bash train_enhanced.sh
```

## ğŸ”§ Tham sá»‘ quan trá»ng

### **Model Configuration:**
- `--model_name`: `pvig_ti_224_gelu`, `pvig_s_224_gelu`, `pvig_m_224_gelu`, `pvig_b_224_gelu`
- `--text_model`: Sentence-BERT model name
- `--embedding_dim`: Dimension cá»§a output embeddings (512, 768, 1024)
- `--freeze_text_encoder`: Freeze Sentence-BERT weights

### **Training Settings:**
- `--batch_size`: Batch size (32 cho GPU 24GB)
- `--lr`: Learning rate (1e-4 recommended)
- `--weight_decay`: Weight decay (0.05)
- `--warmup_epochs`: Warmup epochs (5)
- `--gradient_clip_val`: Gradient clipping (1.0)

### **Loss Configuration:**
- `--temperature`: Temperature cho contrastive loss (0.07)
- `--lambda_local`: Weight cho local alignment loss (0.1)
- `--lambda_global`: Weight cho global consistency loss (0.05)

### **Data Augmentation:**
- `--augmentation`: `light`, `medium`, `strong`

## ğŸ“Š Evaluation & Testing

### **Load vÃ  test model:**
```bash
python test_enhanced_model.py \
    --model_path "outputs/best_model.pth" \
    --config_path "outputs/config.json" \
    --test_type retrieval \
    --image_dir "/path/to/test/images" \
    --text_file "/path/to/test/captions.txt" \
    --max_samples 1000
```

### **Image-to-Text Retrieval:**
```bash
python test_enhanced_model.py \
    --model_path "outputs/best_model.pth" \
    --test_type retrieval \
    --image_path "/path/to/image.jpg" \
    --text_file "/path/to/candidates.txt" \
    --top_k 5
```

### **Text-to-Image Retrieval:**
```bash
python test_enhanced_model.py \
    --model_path "outputs/best_model.pth" \
    --test_type retrieval \
    --query_text "A beautiful sunset over the ocean" \
    --image_dir "/path/to/images" \
    --top_k 5
```

### **Encode embeddings:**
```bash
python test_enhanced_model.py \
    --model_path "outputs/best_model.pth" \
    --test_type encode \
    --image_path "/path/to/image.jpg" \
    --query_text "Your caption here"
```

## ğŸ“ˆ Monitoring

### **WandB Integration:**
- Tá»± Ä‘á»™ng log training metrics, loss components, retrieval performance
- Visualizations: loss curves, similarity matrices, embedding distributions

### **Metrics Ä‘Æ°á»£c track:**
- **Training**: total_loss, contrastive_loss, local_loss, global_loss, learning_rate
- **Validation**: I2T R@1,5,10, T2I R@1,5,10, mean_recall, RSum

## ğŸ’¾ Output Structure

```
outputs/
â”œâ”€â”€ config.json              # Training configuration
â”œâ”€â”€ best_model.pth           # Best model checkpoint
â”œâ”€â”€ checkpoint_epoch_X.pth   # Regular checkpoints
â””â”€â”€ logs/                    # Training logs
```

### **Model checkpoint format:**
```python
{
    'image_encoder': state_dict,
    'text_encoder': state_dict,
    'optimizer': state_dict,
    'scheduler': state_dict,
    'epoch': int,
    'metrics': dict,
    'args': dict
}
```

## ğŸ”§ Memory Management

### **GPU Memory optimization:**
- Mixed precision training (AMP)
- Gradient checkpointing
- Efficient data loading vá»›i pin_memory
- Batch size tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh theo GPU memory

### **Recommended settings:**
- **GTX 1080Ti (11GB)**: batch_size=16, pvig_ti
- **RTX 3080 (10GB)**: batch_size=24, pvig_s  
- **RTX 3090 (24GB)**: batch_size=32, pvig_s/m
- **A100 (40GB)**: batch_size=64, pvig_b

## ğŸ¯ Performance Benchmarks

### **Expected Results (COCO 5K test):**
| Model | Text Encoder | I2T R@1 | I2T R@5 | T2I R@1 | T2I R@5 | RSum |
|-------|--------------|---------|---------|---------|---------|------|
| PVigS-Ti | MiniLM-L6 | 45-50% | 75-80% | 35-40% | 65-70% | 220-240 |
| PVigS-S | MiniLM-L6 | 50-55% | 80-85% | 40-45% | 70-75% | 240-260 |
| PVigS-S | MPNet-Base | 55-60% | 85-90% | 45-50% | 75-80% | 260-280 |

## ğŸ› Troubleshooting

### **Common Issues:**

#### **1. Out of Memory:**
```bash
# Giáº£m batch size
--batch_size 16

# Sá»­ dá»¥ng model nhá» hÆ¡n
--model_name pvig_ti_224_gelu

# Freeze text encoder
--freeze_text_encoder
```

#### **2. Slow training:**
```bash
# Giáº£m sá»‘ workers
--num_workers 2

# Sá»­ dá»¥ng text model nhá» hÆ¡n
--text_model all-MiniLM-L6-v2
```

#### **3. Poor convergence:**
```bash
# Äiá»u chá»‰nh learning rate
--lr 5e-5

# TÄƒng warmup
--warmup_epochs 10

# Äiá»u chá»‰nh loss weights
--lambda_local 0.05 --lambda_global 0.02
```

## ğŸ“ Tips & Best Practices

### **Training Tips:**
1. **Start small**: Test vá»›i `--max_samples 1000` trÆ°á»›c
2. **Monitor convergence**: Kiá»ƒm tra validation metrics má»—i 5 epochs
3. **Use pretrained weights**: LuÃ´n load pretrained ViGNN weights
4. **Batch size**: CÃ ng lá»›n cÃ ng tá»‘t (trong giá»›i háº¡n memory)
5. **Learning rate**: Start vá»›i 1e-4, scale theo batch size

### **Data Preparation:**
1. **Caption quality**: Filter captions quÃ¡ ngáº¯n/dÃ i
2. **Image resolution**: Resize vá» 224x224 trÆ°á»›c training
3. **Balance dataset**: Äáº£m báº£o diverse captions
4. **Text cleaning**: Remove HTML tags, normalize whitespace

### **Inference Optimization:**
1. **Batch inference**: Encode nhiá»u images/texts cÃ¹ng lÃºc
2. **Cache embeddings**: Save embeddings to disk cho reuse
3. **GPU utilization**: Use GPU cho inference khi cÃ³ thá»ƒ

## ğŸ”— Related Work

- **Original ViG**: [Vision GNN: An Image is Worth Graph of Nodes](https://arxiv.org/abs/2206.00272)
- **Sentence-BERT**: [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
- **CLIP**: [Learning Transferable Visual Representations](https://arxiv.org/abs/2103.00020)

## ğŸ“„ License

This project is based on the original ViG implementation and follows the same license terms.

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Commit changes
4. Push to branch
5. Create Pull Request

## ğŸ“ Support

Náº¿u gáº·p váº¥n Ä‘á»:
1. Check Issues trÃªn GitHub
2. Review troubleshooting section
3. Create new issue vá»›i detailed logs 