{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/vig_pytorch/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from gcn_lib import act_layer\n",
    "import torch\n",
    "from torch.nn import Sequential as Seq\n",
    "from timm.models.layers import DropPath\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stem(nn.Module):\n",
    "    \"\"\" Image to Visual Word Embedding\n",
    "    Overlap: https://arxiv.org/pdf/2106.13797.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, in_dim=3, out_dim=768, act='relu'):\n",
    "        super().__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, out_dim//8, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_dim//8),\n",
    "            act_layer(act),\n",
    "            nn.Conv2d(out_dim//8, out_dim//4, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_dim//4),\n",
    "            act_layer(act),\n",
    "            nn.Conv2d(out_dim//4, out_dim//2, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_dim//2),\n",
    "            act_layer(act),\n",
    "            nn.Conv2d(out_dim//2, out_dim, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            act_layer(act),\n",
    "            nn.Conv2d(out_dim, out_dim, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act='relu', drop_path=0.0):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_features, hidden_features, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "        )\n",
    "        self.act = act_layer(act)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, out_features, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_features),\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop_path(x) + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn_lib.torch_nn import BasicConv, batched_index_select, act_layer\n",
    "from gcn_lib.torch_edge import DenseDilatedKnnGraph\n",
    "\n",
    "class MRConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Max-Relative Graph Convolution (Paper: https://arxiv.org/abs/1904.03751) for dense data type\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n",
    "        super(MRConv2d, self).__init__()\n",
    "        self.nn = BasicConv([in_channels*2, out_channels], act, norm, bias)\n",
    "\n",
    "    def forward(self, x, edge_index, y=None):\n",
    "        x_i = batched_index_select(x, edge_index[1])\n",
    "        if y is not None:\n",
    "            x_j = batched_index_select(y, edge_index[0])\n",
    "        else:\n",
    "            x_j = batched_index_select(x, edge_index[0])\n",
    "        x_j, _ = torch.max(x_j - x_i, -1, keepdim=True)\n",
    "        b, c, n, _ = x.shape\n",
    "        x = torch.cat([x.unsqueeze(2), x_j.unsqueeze(2)], dim=2).reshape(b, 2 * c, n, _)\n",
    "        return self.nn(x)\n",
    "    \n",
    "class GraphConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Static graph convolution layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, conv='edge', act='relu', norm=None, bias=True):\n",
    "        super(GraphConv2d, self).__init__()\n",
    "        if conv == 'edge':\n",
    "            self.gconv = EdgeConv2d(in_channels, out_channels, act, norm, bias)\n",
    "        elif conv == 'mr':\n",
    "            self.gconv = MRConv2d(in_channels, out_channels, act, norm, bias)\n",
    "        elif conv == 'sage':\n",
    "            self.gconv = GraphSAGE(in_channels, out_channels, act, norm, bias)\n",
    "        elif conv == 'gin':\n",
    "            self.gconv = GINConv2d(in_channels, out_channels, act, norm, bias)\n",
    "        else:\n",
    "            raise NotImplementedError('conv:{} is not supported'.format(conv))\n",
    "\n",
    "    def forward(self, x, edge_index, y=None):\n",
    "        return self.gconv(x, edge_index, y)\n",
    "\n",
    "\n",
    "class DyGraphConv2d(GraphConv2d):\n",
    "    \"\"\"\n",
    "    Dynamic graph convolution layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=9, dilation=1, conv='edge', act='relu',\n",
    "                 norm=None, bias=True, stochastic=False, epsilon=0.0, r=1):\n",
    "        super(DyGraphConv2d, self).__init__(in_channels, out_channels, conv, act, norm, bias)\n",
    "        self.k = kernel_size\n",
    "        self.d = dilation\n",
    "        self.r = r\n",
    "        self.dilated_knn_graph = DenseDilatedKnnGraph(kernel_size, dilation, stochastic, epsilon)\n",
    "\n",
    "    def forward(self, x, relative_pos=None):\n",
    "        B, C, H, W = x.shape\n",
    "        y = None\n",
    "        if self.r > 1:\n",
    "            y = F.avg_pool2d(x, self.r, self.r)\n",
    "            y = y.reshape(B, C, -1, 1).contiguous()            \n",
    "        x = x.reshape(B, C, -1, 1).contiguous()\n",
    "        edge_index = self.dilated_knn_graph(x, y, relative_pos)\n",
    "        x = super(DyGraphConv2d, self).forward(x, edge_index, y)\n",
    "        return x.reshape(B, -1, H, W).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grapher(nn.Module):\n",
    "    \"\"\"\n",
    "    Grapher module with graph convolution and fc layers\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, kernel_size=9, dilation=1, conv='edge', act='relu', norm=None,\n",
    "                 bias=True,  stochastic=False, epsilon=0.0, r=1, n=196, drop_path=0.0, relative_pos=False):\n",
    "        super(Grapher, self).__init__()\n",
    "        self.channels = in_channels\n",
    "        self.n = n\n",
    "        self.r = r\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "        )\n",
    "        self.graph_conv = DyGraphConv2d(in_channels, in_channels * 2, kernel_size, dilation, conv,\n",
    "                              act, norm, bias, stochastic, epsilon, r)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, in_channels, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.relative_pos = None\n",
    "        if relative_pos:\n",
    "            print('using relative_pos')\n",
    "            relative_pos_tensor = torch.from_numpy(np.float32(get_2d_relative_pos_embed(in_channels,\n",
    "                int(n**0.5)))).unsqueeze(0).unsqueeze(1)\n",
    "            relative_pos_tensor = F.interpolate(\n",
    "                    relative_pos_tensor, size=(n, n//(r*r)), mode='bicubic', align_corners=False)\n",
    "            self.relative_pos = nn.Parameter(-relative_pos_tensor.squeeze(1), requires_grad=False)\n",
    "\n",
    "    def _get_relative_pos(self, relative_pos, H, W):\n",
    "        if relative_pos is None or H * W == self.n:\n",
    "            return relative_pos\n",
    "        else:\n",
    "            N = H * W\n",
    "            N_reduced = N // (self.r * self.r)\n",
    "            return F.interpolate(relative_pos.unsqueeze(0), size=(N, N_reduced), mode=\"bicubic\").squeeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _tmp = x\n",
    "        print(\"============x=============\")\n",
    "        print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        print(\"============fc1=============\")\n",
    "        print(x.shape)\n",
    "        B, C, H, W = x.shape\n",
    "        relative_pos = self._get_relative_pos(self.relative_pos, H, W)\n",
    "        # print(\"============relative_pos=============\")\n",
    "        # print(relative_pos)\n",
    "        x = self.graph_conv(x, relative_pos)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop_path(x) + _tmp\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepGCN(torch.nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(DeepGCN, self).__init__()\n",
    "        channels = opt.n_filters\n",
    "        k = opt.k\n",
    "        act = opt.act\n",
    "        norm = opt.norm\n",
    "        bias = opt.bias\n",
    "        epsilon = opt.epsilon\n",
    "        stochastic = opt.use_stochastic\n",
    "        conv = opt.conv\n",
    "        self.n_blocks = opt.n_blocks\n",
    "        drop_path = opt.drop_path\n",
    "        \n",
    "        self.stem = Stem(out_dim=channels, act=act)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path, self.n_blocks)]  # stochastic depth decay rule \n",
    "        print('dpr', dpr)\n",
    "        num_knn = [int(x.item()) for x in torch.linspace(k, 2*k, self.n_blocks)]  # number of knn's k\n",
    "        print('num_knn', num_knn)\n",
    "        max_dilation = 196 // max(num_knn)\n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, channels, 14, 14))\n",
    "\n",
    "        if opt.use_dilation:\n",
    "            self.backbone = Seq(*[Seq(Grapher(channels, num_knn[i], min(i // 4 + 1, max_dilation), conv, act, norm,\n",
    "                                                bias, stochastic, epsilon, 1, drop_path=dpr[i]),\n",
    "                                      FFN(channels, channels * 4, act=act, drop_path=dpr[i])\n",
    "                                     ) for i in range(self.n_blocks)])\n",
    "        else:\n",
    "            self.backbone = Seq(*[Seq(Grapher(channels, num_knn[i], 1, conv, act, norm,\n",
    "                                                bias, stochastic, epsilon, 1, drop_path=dpr[i]),\n",
    "                                      FFN(channels, channels * 4, act=act, drop_path=dpr[i])\n",
    "                                     ) for i in range(self.n_blocks)])\n",
    "\n",
    "        self.prediction = Seq(nn.Conv2d(channels, 1024, 1, bias=True),\n",
    "                              nn.BatchNorm2d(1024),\n",
    "                              act_layer(act),\n",
    "                              nn.Dropout(opt.dropout),\n",
    "                              nn.Conv2d(1024, opt.n_classes, 1, bias=True))\n",
    "        self.model_init()\n",
    "\n",
    "    def model_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "                m.weight.requires_grad = True\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    m.bias.requires_grad = True\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.stem(inputs) + self.pos_embed\n",
    "        B, C, H, W = x.shape\n",
    "        print('stem', x.shape)\n",
    "        for i in range(self.n_blocks):\n",
    "            x = self.backbone[i](x)\n",
    "            print('block', i, x.shape)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        return self.prediction(x).squeeze(-1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "dummy_image = torch.rand(3, 3, 224, 224)\n",
    "stem = Stem()\n",
    "a = stem(dummy_image)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptInit:\n",
    "    def __init__(self, num_classes=1000, drop_path_rate=0.0, drop_rate=0.0, num_knn=9, **kwargs):\n",
    "        self.k = 9 # neighbor num (default:9)\n",
    "        self.conv = 'mr' # graph conv layer {edge, mr}\n",
    "        self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n",
    "        self.norm = 'batch' # batch or instance normalization {batch, instance}\n",
    "        self.bias = True # bias of conv layer True or False\n",
    "        self.n_blocks = 12 # number of basic blocks in the backbone\n",
    "        self.n_filters = 192 # number of channels of deep features\n",
    "        self.n_classes = 1000 # Dimension of out_channels\n",
    "        self.dropout = 0.2 # dropout rate\n",
    "        self.use_dilation = True # use dilated knn or not\n",
    "        self.epsilon = 0.2 # stochastic epsilon for gcn\n",
    "        self.use_stochastic = False # stochastic for gcn, True or False\n",
    "        self.drop_path = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpr [0.0, 0.0181818176060915, 0.036363635212183, 0.05454545468091965, 0.072727270424366, 0.09090908616781235, 0.10909091681241989, 0.12727272510528564, 0.1454545557498932, 0.16363637149333954, 0.1818181872367859, 0.20000000298023224]\n",
      "num_knn [9, 9, 10, 11, 12, 13, 13, 14, 15, 16, 17, 18]\n",
      "stem torch.Size([3, 192, 14, 14])\n",
      "============x=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "============fc1=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "x.shape, y.shape torch.Size([3, 192, 196, 1])\n",
      "block 0 torch.Size([3, 192, 14, 14])\n",
      "============x=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "============fc1=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "x.shape, y.shape torch.Size([3, 192, 196, 1])\n",
      "block 1 torch.Size([3, 192, 14, 14])\n",
      "============x=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "============fc1=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "x.shape, y.shape torch.Size([3, 192, 196, 1])\n",
      "block 2 torch.Size([3, 192, 14, 14])\n",
      "============x=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "============fc1=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "x.shape, y.shape torch.Size([3, 192, 196, 1])\n",
      "block 3 torch.Size([3, 192, 14, 14])\n",
      "============x=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "============fc1=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "x.shape, y.shape torch.Size([3, 192, 196, 1])\n",
      "block 4 torch.Size([3, 192, 14, 14])\n",
      "============x=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "============fc1=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "x.shape, y.shape torch.Size([3, 192, 196, 1])\n",
      "block 5 torch.Size([3, 192, 14, 14])\n",
      "============x=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "============fc1=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "x.shape, y.shape torch.Size([3, 192, 196, 1])\n",
      "block 6 torch.Size([3, 192, 14, 14])\n",
      "============x=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "============fc1=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "x.shape, y.shape torch.Size([3, 192, 196, 1])\n",
      "block 7 torch.Size([3, 192, 14, 14])\n",
      "============x=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "============fc1=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "x.shape, y.shape torch.Size([3, 192, 196, 1])\n",
      "block 8 torch.Size([3, 192, 14, 14])\n",
      "============x=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "============fc1=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "x.shape, y.shape torch.Size([3, 192, 196, 1])\n",
      "block 9 torch.Size([3, 192, 14, 14])\n",
      "============x=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "============fc1=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "x.shape, y.shape torch.Size([3, 192, 196, 1])\n",
      "block 10 torch.Size([3, 192, 14, 14])\n",
      "============x=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "============fc1=============\n",
      "torch.Size([3, 192, 14, 14])\n",
      "x.shape, y.shape torch.Size([3, 192, 196, 1])\n",
      "block 11 torch.Size([3, 192, 14, 14])\n",
      "torch.Size([3, 1000])\n"
     ]
    }
   ],
   "source": [
    "opt = OptInit()\n",
    "depp_gcn = DeepGCN(opt=opt)\n",
    "b = depp_gcn(dummy_image)\n",
    "print(b.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vig_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b190c41a14553fb78f0dbb9ff609c5b855439fa8350031ad19a757d5d156c7ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
